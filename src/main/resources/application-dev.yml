# 配置文件的内容都可以配置在阿波罗动态配置
# 阿波罗配置
#app:
#  id: spring-mvc-jef
#apollo:
#  # 注册路径，阿波罗默认注册配置 Eureka
#  meta: http://localhost:8080
#  bootstrap:
#    enabled: true
#    # 指定阿波罗中配置项名称，多个用逗号隔开
#    namespaces: application

# 配置文件
eureka:
  client:
    # 表示将自己注册进Eureka Server默认为true
    register-with-eureka: true
    # 是否从Eureka Server抓去已有的注册信息，默认是true
    fetch-registry: true
    # 设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址
    service-url:
      defaultZone: http://localhost:8080/eureka

spring:
  thymeleaf:
    # controller 返回 String 后跳转到指定界面时的路径前缀，下面路径的两个 / 都不能省略
    prefix: classpath:/static/
  mvc:
    static-path-pattern: /**
  resources:
    # 静态资源的默认访问路径前缀
    static-locations: classpath:/static
  application:
    name: cloud-payment-service
  # 数据源基本配置
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/all_test?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC
    username: root
    password: Root@123
  config:
    import: nacos/message.yaml
  kafka:
    bootstrap-servers: localhost:9092
    producer: # producer 生产者
      retries: 0 # 重试次数
      acks: 1 # 应答级别:多少个分区副本备份完成时向生产者发送ack确认(可选0、1、all/-1)
      batch-size: 16384 # 批量大小
      buffer-memory: 33554432 # 生产端缓冲区大小
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer: # consumer消费者
      group-id: jef-group # 默认的消费组ID
      enable-auto-commit: true # 是否自动提交offset
      auto-commit-interval: 100  # 提交offset延时(接收到消息后多久提交offset)
      # earliest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
      # latest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
      # none:topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
      auto-offset-reset: latest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
  redis:
    database: 0
    host: 127.0.0.1
    port: 6379
    timeout: 5000
    password: root
    lettuce:
      pool:
        max-active: 32
        max-wait: -1
        max-idle: 16
        min-idle: 8

  shardingsphere:
    datasource:
      # 定义两个全局数据源
      names: ds-0,ds-1
      # 配置数据源 ds-0
      ds-0:
        type: com.alibaba.druid.pool.DruidDataSource
        driverClassName: com.mysql.jdbc.Driver
        url: jdbc:mysql://localhost:3306/ds-0?useUnicode=true&characterEncoding=utf8&tinyInt1isBit=false&useSSL=false&serverTimezone=GMT
        username: root
        password: Root@123
      # 配置数据源 ds-1
      ds-1:
        type: com.alibaba.druid.pool.DruidDataSource
        driverClassName: com.mysql.jdbc.Driver
        url: jdbc:mysql://localhost:3306/ds-1?useUnicode=true&characterEncoding=utf8&tinyInt1isBit=false&useSSL=false&serverTimezone=GMT
        username: root
        password: Root@123
    # 默认数据源，未分片的表默认执行库
    sharding:
      default-data-source-name: ds-0
      # 分库策略
      tables:
        t_order:
          database-strategy:
            standard:
              # 分库分片健
              sharding-column: order_id
              # 分库分片算法
              precise-algorithm-class-name: com.jef.common.sharding.MyDBPreciseShardingAlgorithm

          # 分表策略
          table-strategy:
            standard:
              # 分表分片健
              sharding-column: order_id
              # 分表算法
              precise-algorithm-class-name: com.jef.common.sharding.MyTablePreciseShardingAlgorithm

            # 配置分片表 t_order
            # 指定真实数据节点
          actual-data-nodes: ds-$->{0..1}.t_order_$->{0..2}
  # 中断循环依赖
  main:
    allow-circular-references: true
# 端口
server:
  port: 8080

mybatis:
  mapperLocations: classpath:mapper/*Mapper.xml
  # 所有entity别名类所在的包
  type-aliases-pachage: com.jef.entity

# 打印sql
logging:
  level:
    com.jef.dao : debug

dubbo:
  application:
    name: springboot-dubbo-demotest #应用名
  registry:
    #    address: zookeeper://127.0.0.1:2181 #zookeeper地址，启动zookeeper后使用这个
    address: N/A #zookeeper地址
    check: false
  #   port: 2181 #提供注册的端口
  # 采用协议方式和端口号
  protocol:
    name: dubbo # 采用dubbo协议
    port: "20890"# dubbo服务暴露的端口
  scan:
    base-packages: com.jef.service.impl #扫描的实现类包名
#  service:
#    check: false
#  reference:
#    check: false
#  config-center:
#    timeout: 25000

liteflow:
  #规则文件路径
  rule-source: config/*.el.yml
  #-----------------以下非必须-----------------
  #liteflow是否开启，默认为true
  enable: true
  #liteflow的banner打印是否开启，默认为true
  print-banner: true
  #zkNode的节点，只有使用zk作为配置源的时候才起作用，默认为/lite-flow/flow
  zk-node: /lite-flow/flow
  #上下文的最大数量槽，默认值为1024
  slot-size: 1024
  #FlowExecutor的execute2Future的线程数，默认为64
  main-executor-works: 64
  #FlowExecutor的execute2Future的自定义线程池Builder，LiteFlow提供了默认的Builder
  main-executor-class: com.yomahub.liteflow.thread.LiteFlowDefaultMainExecutorBuilder
  #自定义请求ID的生成类，LiteFlow提供了默认的生成类
  request-id-generator-class: com.yomahub.liteflow.flow.id.DefaultRequestIdGenerator
  #并行节点的线程池Builder，LiteFlow提供了默认的Builder
  thread-executor-class: com.yomahub.liteflow.thread.LiteFlowDefaultWhenExecutorBuilder
  #异步线程最长的等待时间(只用于when)，默认值为15000
  when-max-wait-time: 15000
  #异步线程最长的等待时间(只用于when)，默认值为MILLISECONDS，毫秒
  when-max-wait-time-unit: MILLISECONDS
  #when节点全局异步线程池最大线程数，默认为16
  when-max-workers: 16
  #when节点全局异步线程池等待队列数，默认为512
  when-queue-limit: 512
  #是否在启动的时候就解析规则，默认为true
  parse-on-start: true
  #全局重试次数，默认为0
  retry-count: 0
  #是否支持不同类型的加载方式混用，默认为false
  support-multiple-type: false
  #全局默认节点执行器
  node-executor-class: com.yomahub.liteflow.flow.executor.DefaultNodeExecutor
  #是否打印执行中过程中的日志，默认为true
  print-execution-log: true
  #是否开启本地文件监听，默认为false
  enable-monitor-file: false
  #简易监控配置选项
  monitor:
    #监控是否开启，默认不开启
    enable-log: false
    #监控队列存储大小，默认值为200
    queue-limit: 200
    #监控一开始延迟多少执行，默认值为300000毫秒，也就是5分钟
    delay: 300000
    #监控日志打印每过多少时间执行一次，默认值为300000毫秒，也就是5分钟
    period: 300000

# 配置中心配置
#  rule-source-ext-data-map:
#    serverAddr: 127.0.0.1:8848
#    dataId: demo_rule
#    group: jef-application
#    namespace: dev
#    username: nacos
#    password: nacos


rocketmq:
  consumer:
    group: springboot_consumer_group
    # 一次拉取消息最大值，注意是拉取消息的最大值而非消费最大值
    pull-batch-size: 10
  name-server: 127.0.0.1:9876
  producer:
    # 发送同一类消息的设置为同一个group，保证唯一
    group: springboot_producer_group
    # 发送消息超时时间，默认3000
    sendMessageTimeout: 10000
    # 发送消息失败重试次数，默认2
    retryTimesWhenSendFailed: 2
    # 异步消息重试此处，默认2
    retryTimesWhenSendAsyncFailed: 2
    # 消息最大长度，默认1024 * 1024 * 4(默认4M)
    maxMessageSize: 4096
    # 压缩消息阈值，默认4k(1024 * 4)
    compressMessageBodyThreshold: 4096
    # 是否在内部发送失败时重试另一个broker，默认false
    retryNextServer: false

elasticsearch:
  ip: 127.0.0.1
  port: 9300
  pool-size: 5
  cluster:
    name: elasticsearch
  url: http://127.0.0.1:9200
  username: # 不存在就留空
  password:

kafka:
  topic:
    my-topic: my-topic
    my-topic2: my-topic2

# Seata 配置
seata:
  enabled: true # 默认开启
  application-id: ${spring.application.name}
  tx-service-group: default_tx_group # 事务群组（可以每个应用独立取名，也可以使用相同的名字）
  client:
    rm-report-success-enable: true
    rm-table-meta-check-enable: false # 自动刷新缓存中的表结构（默认false）
    rm-report-retry-count: 5 # 一阶段结果上报TC重试次数（默认5）
    rm-async-commit-buffer-limit: 10000 # 异步提交缓存队列长度（默认10000）
    rm:
      lock:
        lock-retry-internal: 10 # 校验或占用全局锁重试间隔（默认10ms）
        lock-retry-times: 30 # 校验或占用全局锁重试次数（默认30）
        lock-retry-policy-branch-rollback-on-conflict: true # 分支事务与其它全局回滚事务冲突时锁策略（优先释放本地锁让回滚成功）
    tm-commit-retry-count: 3 # 一阶段全局提交结果上报TC重试次数（默认1次，建议大于1）
    tm-rollback-retry-count: 3 # 一阶段全局回滚结果上报TC重试次数（默认1次，建议大于1）
    undo:
      undo-data-validation: true # 二阶段回滚镜像校验（默认true开启）
      undo-log-serialization: jackson # undo序列化方式（默认jackson）
      undo-log-table: undo_log  # 自定义undo表名（默认undo_log）
    log:
      exception-rate: 100 # 日志异常输出概率（默认100）
    support:
      spring:
        datasource-autoproxy: true
  service:
    vgroup-mapping:
      default_tx_group: default # TC 集群（必须与seata-server保持一致）
    grouplist:
      default: 127.0.0.1:8080
    enable-degrade: false # 降级开关
    disable-global-transaction: false # 禁用全局事务（默认false）

  transport:
    type: TCP
    server: NIO
    heartbeat: true
    serialization: seata
    compressor: none
    enable-client-batch-send-request: true # 客户端事务消息请求是否批量合并发送（默认true）
    shutdown:
      wait: 3
    thread-factory:
      boss-thread-prefix: NettyBoss
      worker-thread-prefix: NettyServerNIOWorker
      server-executor-thread-prefix: NettyServerBizHandler
      share-boss-worker: false
      client-selector-thread-prefix: NettyClientSelector
      client-selector-thread-size: 1
      client-worker-thread-prefix: NettyClientWorkerThread

  registry:
    type: file  # file 、nacos 、eureka、redis、zk、consul、etcd3、sofa
  config:
    #    file:
    #      name: file.conf
    type: file  # file、nacos 、apollo、zk、consul、etcd3

